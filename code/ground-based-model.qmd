---
title: "ground-based-model"
output: html_document
date: "2023-12-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load all packages and remove all objects from memory

```{r}
rm(list = ls())
gc()
```

```{r Load packages, warn=FALSE, message=FALSE}
library(rpart.plot)
library(tidyverse)
library(tidymodels)
library(vip)
library(glue)
library(tictoc)
library(xgboost)
source('code/src/spatial-utils.R')
```

## Set the seed for reproducibility

```{r Set the seed}
set.seed(8675309)
```

## Extract all pixels

If the data is not generated run 'src/sample-all-pixels.R' to extract pixel data from all 50 photographs using the labeled polygons

You can use the function `extract_polygon_pixels` to extract the pixels from polygons drawn in `label-me`. The function takes four arguments:

1.  the path to the image
2.  a polygon sf object or the path to the polygons json file
3.  a boolean to include the polygon data (IDs of polygons etc)
4.  the extent of the image

The function returns a list of data frames with the pixels from the polygons.

```{r}
extract_polygon_pixels
```

```{r}
pixels_path <- 'clean_data/labeled_pixels.rds'

clean_pixels <- readRDS(pixels_path) |>
  select(!contains("hsl") | contains("_3") | contains("_L3")) |> #drop reduntant hs values
  group_by(key) |>
  slice_sample(n = 1000) |>
  ungroup()

clean_pixels
```

```{r}
clean_pixels %>% 
  count(label) %>% 
  mutate(prop = n/sum(n))
```


## Create training and testing set

```{r}
pixel_split <- clean_pixels |>
  dplyr::select(-c(cell))|>
  mutate(label = as.factor(label)) |>
  group_initial_split(prop = 0.75, group = key)

pixels_train <- pixel_split |>
  training()

pixels_test <- pixel_split |>
  testing()
```

```{r}
pixels_train |>
  head()
pixels_test |> 
  head()
```

## XGBoost model

First we build the formula with all the predictors

```{r}
all_columns <- names(clean_pixels)# |>
  #sample(size =  110)

non_predictors <- c("cell", "train_val", "label", "directory", "poly_num", "key", "label", "area")
predictors <- all_columns[!all_columns %in% non_predictors]

#formula <- as.formula(paste("label ~", paste(predictors, collapse = " + ")))
#formula <- as.formula(paste("label ~ hsv_1"))

predictors
```

We have hundreds of predictors so we fit a single decision tree to get the top 20 layers.

```{r}
tree_spec <- decision_tree() |>
  set_engine("rpart") |>
  set_mode("classification") |>
  set_args(cost_complexity = tune())

data <- pixels_train |>
  dplyr::select(all_of(c(predictors, 'label') ))

# We use update_role because formula eats up ram usage  if there are many predictors(will throw an error)
# https://github.com/tidymodels/recipes/issues/548

formula<- as.formula(paste("label ~", paste(predictors, collapse = " + ")))
recipe <- recipe(label ~ ., data = pixels_train)

tree_rec <- recipe(data) |> 
  update_role(everything()) |>
  update_role(label, new_role = "outcome") |> 
  step_naomit(all_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_predictors())

tree_wf <- workflow() |>
  add_model(tree_spec) |>
  add_recipe(tree_rec)

tree_grid <- grid_regular(cost_complexity(range = c(-10, -1)), levels = 20)
```

Tune the decision tree

```{r Tuning decision tree}
pixel_folds <- pixels_train |>
  group_vfold_cv(v = 10, group = key, balance = "observations")

tic()
doParallel::registerDoParallel(15)
tune_res <- tune_grid(
  tree_wf,
  resamples = pixel_folds,
  grid = tree_grid,
  control = control_grid(verbose = TRUE)
)
toc()
```

Visualize tuning results

```{r}
tune_res |>
  autoplot(metric = "accuracy") +
  theme_minimal()
```

```{r}
tree_best <- tune_res |>
  select_by_one_std_err(desc(cost_complexity), metric = "accuracy")

tree_final <- tree_wf |>
  finalize_workflow(tree_best)

tree_fit <- tree_final |>
  fit(data = pixels_train)

# with usual 0.5 threshold
tree_fit |>
  augment(new_data = pixels_test) |>
  conf_mat(truth = label, estimate = .pred_class)

importance <- tree_fit |>
  extract_fit_parsnip() |>
  vip(num_features = 15)
importance
```

```{r}
tree_fit |>
  extract_fit_engine() |>
  rpart.plot(box.palette = list("gray80", "darkolivegreen3", "gold2"), tweak = 0.5)
```

```{r}
top_features <- importance$data$Variable
formula_top_features <- 
  as.formula(paste("label ~", paste(top_features, collapse = " + ")))
formula_top_features
```

```{r}
xgb_specification <- boost_tree(
  trees = 1000,
  min_n = tune(),
  mtry = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) |>
  set_engine("xgboost", nthread = 5) |>
  set_mode("classification")

data <- pixels_train[c(predictors, "label")]

# We use update_role because formula eats up ram usage if there are too many predictors(will throw an error)
# https://github.com/tidymodels/recipes/issues/548

xgb_recipe  <- recipe(data) |> 
  update_role(everything()) |>
  update_role(label, new_role = "outcome") |> 
  step_naomit(all_predictors()) |>
  step_novel(all_nominal_predictors()) |>
  step_dummy(all_nominal_predictors()) |>
  step_zv(all_predictors()) |>
  step_normalize(all_predictors())
  
xgb_workflow <- workflow() |>
  add_model(xgb_specification) |>
  add_recipe(xgb_recipe)

prepped_recipe <- prep(xgb_recipe)
baked_recipe <- bake(prepped_recipe, new_data = NULL)

xgb_grid <- grid_latin_hypercube(
  finalize(mtry(), baked_recipe),
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  size = 12
)
```

## Create folds for K fold cross validation

```{r}
pixel_folds <- pixels_train |>
  group_vfold_cv(v = 10, group = key, balance = "observations")
```

Try nthread = 3 or 4 as an option in set_engine. Then try registerDoParallel(8) or 6. If runs out of memeory use more threads and fewer forks.

Can also try racing methods to really speed things up.

```{r Training XGBoost model}
tic()
doParallel::registerDoParallel(15)
tune_results <- tune_grid(
  xgb_workflow,
  resamples = pixel_folds,
  grid = xgb_grid,
  control = control_grid(verbose = TRUE)
  )
toc()
```

Tuning results

```{r Tuning results}
tune_results |>
  collect_metrics() |>
  filter(.metric == "accuracy") |>
  pivot_longer(mtry:sample_size, names_to = "parameter", values_to = "value") |>
  rename(accuracy = mean) |>
  ggplot(aes(value, accuracy, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~ parameter, scales = "free_x") +
  theme_minimal()
```

Train the model with best hyper-parameters based on accuracy and then calculate testing error.

```{r Finalize best model and create workflow}
xgb_best <- tune_results |>
  select_best(metric = "accuracy")

xgb_final <- xgb_workflow |>
  finalize_workflow(xgb_best)

xgb_fit <- xgb_final |>
  fit(data = pixels_train)
```

```{r Save the model}
today <- Sys.Date()
saveRDS(xgb_fit, glue('clean_data/xgb_fit_{today}.rds'))
xgb_fit <- readRDS('clean_data/xgb_fit_2024-02-25.rds')
```

Confusion matrix on the test set

```{r}
# 0.5 threshold
confusion_matrix <- xgb_fit |>
  augment(new_data = pixels_test) |>
  conf_mat(truth = label, estimate = .pred_class)
confusion_matrix
```

```{r}
xgb_fit |>
  augment(new_data = pixels_test) |>
  accuracy(truth = label, estimate = .pred_class)
```

Accuracy on the test set

```{r}
accuracy <- sum(diag(confusion_matrix$table)) / sum(confusion_matrix$table)
accuracy
```

Find the top features

```{r}
xgb_fit |>
  extract_fit_engine() |>
  vip(num_features = 25) +
  theme_minimal()
```

```{r}
top_features <- xgb_fit |>
  extract_fit_engine() |>
  vip(num_features = 25)
top_features$data
```
