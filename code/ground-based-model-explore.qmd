---
title: "ground-based-model-explore"
output: html_document
date: "2024-05-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Load all packages and remove all objects from memory

```{r}
rm(list = ls())
gc()
```

```{r Load packages, warn=FALSE, message=FALSE}
library(tidyverse)
library(tidymodels)
library(vip)
library(glue)
library(tictoc)
library(xgboost)
source('code/src/spatial-utils.R')
```

## Set the seed for reproducibility

```{r Set the seed}
set.seed(8675309)
```

## Extract all pixels

If the data is not generated run 'src/sample-all-pixels.R' to extract pixel data from all 50 photographs using the labeled polygons


```{r}
pixels_path <- 'clean_data/labeled_pixels.rds'

clean_pixels <- readRDS(pixels_path) |>
  select(!contains("hsl") | contains("_3") | contains("_L3")) |> #drop reduntant hs values
  mutate(label = as.factor(label)) |>
  group_by(key) |>
  slice_sample(n = 1000) |>
  ungroup()

clean_pixels
```

```{r}
clean_pixels %>% 
  count(label) %>% 
  mutate(prop = n/sum(n))
```


## Create training and testing set and cross-validation folds

```{r}
pix_split <- clean_pixels |>
  group_initial_split(prop = 0.75, group = key)

pix_train <- pix_split |>
  training()

pix_test <- pix_split |>
  testing()
```

```{r}
pix_folds <- pix_train %>%
  group_vfold_cv(v = 10, group = key, balance = "observations")
```

## XGBoost model

```{r}
xgb_rec  <- recipe(label ~ . , data = pix_train) %>%
  step_rm(key) %>%
  step_rm(cell) %>%
  step_rm(area) %>%
  step_naomit(all_predictors()) %>%
  step_novel(all_nominal_predictors()) %>%
  step_dummy(all_nominal_predictors()) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
```

```{r}
xgb_spec <- boost_tree(
  trees = 1000,
  mtry = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
) %>%
  set_engine("xgboost", nthread = 4) %>%
  set_mode("classification")

xgb_wf <- workflow() %>%
  add_model(xgb_spec) %>%
  add_recipe(xgb_rec)

xgb_grid <- grid_latin_hypercube(
  finalize(mtry(), bake(xgb_rec %>% prep(), new_data = NULL)),
  min_n(),
  tree_depth(),
  learn_rate(),
  loss_reduction(),
  sample_size = sample_prop(),
  size = 24
)
```

```{r}
set.seed(84321)
library(tictoc)
tic()
doParallel::registerDoParallel(6)
tune_res <- tune_grid(
  xgb_wf,
  resamples = pix_folds,
  grid = xgb_grid,
  control = control_grid(verbose = TRUE)
)
toc()
```

Tuning results

```{r}
tune_res %>%
  collect_metrics() %>%
  filter(.metric == "accuracy") %>%
  pivot_longer(mtry:sample_size, names_to = "parameter", values_to = "value") %>%
  rename(accuracy = mean) %>%
  ggplot(aes(value, accuracy, color = parameter)) +
  geom_point(show.legend = FALSE) +
  facet_wrap(~ parameter, scales = "free_x") +
  theme_minimal()
```

```{r}
xgb_best <- tune_res %>%
  select_best(metric = "accuracy")

xgb_final <- xgb_wf %>%
  finalize_workflow(xgb_best)

xgb_fit <- xgb_final %>%
  fit(data = pix_train)

xgb_fit %>%
  augment(new_data = pix_test) %>%
  accuracy(truth = label, estimate = .pred_class)

# with usual 0.5 threshold
xgb_fit %>%
  augment(new_data = pix_test) %>%
  conf_mat(truth = label, estimate = .pred_class)

xgb_fit %>%
  extract_fit_engine() %>%
  vip(num_features = 50, geom = "point") +
  theme_minimal()
```

```{r}
#tune_res %>% saveRDS("clean_data/tune_res_all_pred.rds")
tune_res %>% saveRDS("clean_data/tune_res_all_pred_new.rds")
```